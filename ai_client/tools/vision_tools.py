"""
Vision Tools - –ö–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –¥–ª—è Guardian
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–∞–º–µ—Ä–∞–º–∏, –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ
"""

import os
import cv2
import base64
import logging
import numpy as np
from typing import Optional, Dict, Any, List, Tuple
from datetime import datetime
import json
import requests

from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()
error_handler = ErrorHandler()

class VisionTools:
    """
    –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vision Tools"""
        self.cameras = {}  # –°–ª–æ–≤–∞—Ä—å –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–∞–º–µ—Ä
        self.last_frames = {}  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∫–∞–¥—Ä—ã —Å –∫–∞–º–µ—Ä
        self.motion_detectors = {}  # –î–µ—Ç–µ–∫—Ç–æ—Ä—ã –¥–≤–∏–∂–µ–Ω–∏—è
        self.logger = logger
        self.error_handler = error_handler
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenCV
        try:
            cv2.__version__
            self.logger.info("üì∑ Vision Tools: OpenCV initialized successfully")
        except Exception as e:
            self.logger.error(f"‚ùå Vision Tools: OpenCV initialization failed - {e}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Cloud Vision SDK (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–∏—Å–Ω—ã–π –∞–∫–∫–∞—É–Ω—Ç)
        self._gcv_client = None
        try:
            from google.cloud import vision as gcv
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è GOOGLE_APPLICATION_CREDENTIALS –∑–∞–¥–∞–Ω–∞, –∫–ª–∏–µ–Ω—Ç –ø–æ–¥–Ω–∏–º–µ—Ç—Å—è
            self._gcv_client = gcv.ImageAnnotatorClient()
            self.logger.info("‚úÖ Google Cloud Vision SDK client initialized")
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Google Cloud Vision SDK unavailable: {e}")
    
    def capture_image(self, camera_id: str = "default", auto_analyze: bool = True) -> str:
        """
        –°–¥–µ–ª–∞—Ç—å —Å–Ω–∏–º–æ–∫ —Å –∫–∞–º–µ—Ä—ã
        
        Args:
            camera_id: ID –∫–∞–º–µ—Ä—ã (default, webcam, ip_camera)
            
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–º–µ—Ä—ã
            if camera_id == "default" or camera_id == "webcam":
                source = 0  # –õ–æ–∫–∞–ª—å–Ω–∞—è –≤–µ–±-–∫–∞–º–µ—Ä–∞
            elif camera_id.startswith("rtsp://"):
                source = camera_id  # IP –∫–∞–º–µ—Ä–∞
            else:
                source = int(camera_id) if camera_id.isdigit() else camera_id
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–∞–º–µ—Ä—É
            cap = cv2.VideoCapture(source)
            if not cap.isOpened():
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É {camera_id}"
            
            # –î–µ–ª–∞–µ–º —Å–Ω–∏–º–æ–∫
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä —Å –∫–∞–º–µ—Ä—ã {camera_id}"
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            os.makedirs("memory/captures", exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"memory/captures/capture_{camera_id}_{timestamp}.jpg"
            cv2.imwrite(filename, frame)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏–º–ª–∏–Ω–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–Ω–∏–º–æ–∫
            latest_symlink = f"memory/captures/capture_{camera_id}_latest.jpg"
            try:
                if os.path.islink(latest_symlink) or os.path.exists(latest_symlink):
                    os.remove(latest_symlink)
                os.symlink(os.path.abspath(filename), latest_symlink)
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–∏–º–ª–∏–Ω–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–Ω–∏–º–æ–∫: {e}")
            
            self.logger.info(f"üì∑ Vision Tools: –°–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω - {filename}")
            if auto_analyze:
                try:
                    analysis = self.analyze_image(filename)
                    return f"‚úÖ –°–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}\n{analysis}"
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Auto analysis failed: {e}")
            return f"‚úÖ –°–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filename}"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞—Ö–≤–∞—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"
            self.logger.error(error_msg)
            return error_msg
    
    def analyze_image(self, image_path: str) -> str:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:
        - –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (OpenCV)
        - Google Vision API (labels/text/faces/objects) ‚Äî –µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∫–ª—é—á
        - LLM-–∞–Ω–∞–ª–∏–∑ (Gemini Vision) ‚Äî –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–≤–æ—Ç—ã
        """
        try:
            if not os.path.exists(image_path):
                return f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω"

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = cv2.imread(image_path)
            if image is None:
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_path}"

            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            height, width = image.shape[:2]
            channels = image.shape[2] if len(image.shape) > 2 else 1
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            # –†–µ–∑–∫–æ—Å—Ç—å –∫–∞–∫ –¥–∏—Å–ø–µ—Ä—Å–∏—è –ª–∞–ø–ª–∞—Å–∏–∞–Ω–∞
            sharpness = float(cv2.Laplacian(gray, cv2.CV_64F).var())
            # –Ø—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            brightness = float(np.mean(gray))
            contrast = float(np.std(gray))
            # –°—Ä–µ–¥–Ω–∏–π —Ü–≤–µ—Ç
            avg_bgr = np.mean(np.mean(image, axis=0), axis=0)
            avg_color = {
                "b": float(avg_bgr[0]),
                "g": float(avg_bgr[1]),
                "r": float(avg_bgr[2]),
            }

            basic = {
                "dimensions": f"{width}x{height}",
                "channels": channels,
                "file_size": f"{os.path.getsize(image_path)} bytes",
                "sharpness": round(sharpness, 2),
                "brightness": round(brightness, 2),
                "contrast": round(contrast, 2),
                "avg_color_bgr": avg_color,
                "timestamp": datetime.now().isoformat()
            }

            # Google Vision API (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            gv_summary = None
            try:
                api_key = os.getenv('GOOGLE_CLOUD_VISION_API_KEY')
                if api_key:
                    with open(image_path, 'rb') as f:
                        content_b64 = base64.b64encode(f.read()).decode('utf-8')
                    url = f"https://vision.googleapis.com/v1/images:annotate?key={api_key}"
                    request_data = {
                        "requests": [
                            {
                                "image": {"content": content_b64},
                                "features": [
                                    {"type": "LABEL_DETECTION", "maxResults": 10},
                                    {"type": "TEXT_DETECTION"},
                                    {"type": "FACE_DETECTION"},
                                    {"type": "OBJECT_LOCALIZATION", "maxResults": 10}
                                ]
                            }
                        ]
                    }
                    resp = requests.post(url, json=request_data, timeout=20)
                    if resp.status_code == 200:
                        data = resp.json().get('responses', [{}])[0]
                        labels = [x['description'] for x in data.get('labelAnnotations', [])]
                        objects = [x['name'] for x in data.get('localizedObjectAnnotations', [])]
                        text = data.get('textAnnotations', [{}])[0].get('description', '').strip() if data.get('textAnnotations') else ''
                        faces = len(data.get('faceAnnotations', [])) if data.get('faceAnnotations') else 0
                        gv_summary = {
                            "labels": labels[:10],
                            "objects": objects[:10],
                            "text_sample": text[:200],
                            "faces_detected": faces
                        }
                    else:
                        gv_summary = {"error": f"Vision API status {resp.status_code}"}
            except Exception as e:
                gv_summary = {"error": f"Vision API error: {e}"}

            # LLM-–∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Gemini Vision (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            llm_summary = None
            try:
                from ..models.gemini_client import GeminiClient
                gemini = GeminiClient()
                # –ì—Ä–∞—É–Ω–¥–∏–º LLM —Ñ–∞–∫—Ç–∞–º–∏ –∏–∑ GV –∏ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Files API-–ø—É—Ç—å
                gv_for_prompt = {
                    'labels': (gv_summary or {}).get('labels') if isinstance(gv_summary, dict) else None,
                    'objects': (gv_summary or {}).get('objects') if isinstance(gv_summary, dict) else None,
                    'faces_detected': (gv_summary or {}).get('faces_detected') if isinstance(gv_summary, dict) else None,
                    'text_sample': (gv_summary or {}).get('text_sample') if isinstance(gv_summary, dict) else None,
                }
                prompt_text = (
                    "–û–ø–∏—à–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Ñ–∞–∫—Ç–∞–º.; "
                    "1) –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ —Å–∞–º–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ; 2) –µ—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî 'unknown'; 3) 5-8 –ø—É–Ω–∫—Ç–æ–≤, –±–µ–∑ –≤–æ–¥—ã.\n"
                    f"–î–µ—Ç–µ–∫—Ç–æ—Ä—ã: dimensions={basic['dimensions']}, faces={gv_for_prompt.get('faces_detected')}, "
                    f"labels={gv_for_prompt.get('labels')}, objects={gv_for_prompt.get('objects')}, text_sample={(gv_for_prompt.get('text_sample') or '')[:120]}"
                )
                llm_text = gemini.analyze_image_with_files_api(
                    image_path=image_path,
                    prompt_text=prompt_text,
                    preferred_model="gemini-1.5-pro"
                )
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∏—Å–∫–ª–µ–π–º–µ—Ä–æ–≤
                bad_markers = [
                    'text-based AI', 'cannot directly view', 'cannot see images',
                    'hypothetical', 'I do not have the ability to view images'
                ]
                if any(m.lower() in (llm_text or '').lower() for m in bad_markers):
                    llm_text = None
                llm_summary = llm_text
            except Exception as e:
                llm_summary = f"Gemini analysis unavailable: {e}"

            result = {
                "basic": basic,
                "google_vision": gv_summary,
                "llm": llm_summary
            }

            # –õ–æ–≥ –∏ –≤–æ–∑–≤—Ä–∞—Ç
            analysis_file = image_path.rsplit('.', 1)[0] + '_analysis.json'
            try:
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
            except Exception:
                pass

            self.logger.info(f"üîç Vision Tools: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω - {analysis_file}")
            # –ö—Ä–∞—Ç–∫–∏–π –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è —á–∞—Ç–∞
            summary_lines = [
                f"–†–∞–∑–º–µ—Ä: {basic['dimensions']}, —è—Ä–∫–æ—Å—Ç—å: {basic['brightness']}, –∫–æ–Ω—Ç—Ä–∞—Å—Ç: {basic['contrast']}, —Ä–µ–∑–∫–æ—Å—Ç—å: {basic['sharpness']}",
            ]
            if isinstance(gv_summary, dict) and not gv_summary.get('error'):
                if gv_summary.get('labels'):
                    summary_lines.append("GV labels: " + ", ".join(gv_summary['labels'][:5]))
                if gv_summary.get('objects'):
                    summary_lines.append("GV objects: " + ", ".join(gv_summary['objects'][:5]))
                if gv_summary.get('text_sample'):
                    summary_lines.append("GV text: " + gv_summary['text_sample'].replace('\n',' ')[:120])
                summary_lines.append(f"GV faces: {gv_summary.get('faces_detected', 0)}")
            elif isinstance(gv_summary, dict) and gv_summary.get('error'):
                summary_lines.append(gv_summary['error'])

            if isinstance(llm_summary, str) and llm_summary.strip():
                summary_lines.append("LLM: " + (llm_summary[:400] + ("..." if len(llm_summary) > 400 else "")))

            return "‚úÖ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n" + "\n".join(summary_lines)

        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"
            self.logger.error(error_msg)
            return error_msg
    
    def detect_motion(self, camera_id: str = "default", threshold: float = 25.0) -> str:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è —Å –∫–∞–º–µ—Ä—ã
        
        Args:
            camera_id: ID –∫–∞–º–µ—Ä—ã
            threshold: –ü–æ—Ä–æ–≥ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            str: –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–∞–º–µ—Ä—ã
            if camera_id == "default" or camera_id == "webcam":
                source = 0
            else:
                source = camera_id
            
            cap = cv2.VideoCapture(source)
            if not cap.isOpened():
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É {camera_id}"
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–≤–∞ –∫–∞–¥—Ä–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            ret1, frame1 = cap.read()
            if not ret1:
                cap.release()
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä"
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            import time
            time.sleep(0.1)
            
            ret2, frame2 = cap.read()
            cap.release()
            
            if not ret2:
                return f"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤—Ç–æ—Ä–æ–π –∫–∞–¥—Ä"
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–æ—Å—Ç—å
            diff = cv2.absdiff(gray1, gray2)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
            _, thresh = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)
            
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç—É—Ä—ã
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ç—É—Ä—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É
            motion_detected = False
            for contour in contours:
                if cv2.contourArea(contour) > 500:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å
                    motion_detected = True
                    break
            
            result = {
                "motion_detected": motion_detected,
                "contours_found": len(contours),
                "threshold_used": threshold,
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"üéØ Vision Tools: –î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è - {motion_detected}")
            return f"‚úÖ –î–µ—Ç–µ–∫—Ü–∏—è –¥–≤–∏–∂–µ–Ω–∏—è: {json.dumps(result, ensure_ascii=False)}"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–≤–∏–∂–µ–Ω–∏—è: {e}"
            self.logger.error(error_msg)
            return error_msg
    
    def list_cameras(self) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞–º–µ—Ä
        
        Returns:
            str: –°–ø–∏—Å–æ–∫ –∫–∞–º–µ—Ä
        """
        try:
            cameras = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –∫–∞–º–µ—Ä—ã (0-9)
            for i in range(10):
                cap = cv2.VideoCapture(i)
                if cap.isOpened():
                    ret, frame = cap.read()
                    if ret:
                        cameras.append({
                            "id": str(i),
                            "type": "local",
                            "status": "available"
                        })
                    cap.release()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            cameras.append({
                "id": "default",
                "type": "webcam",
                "status": "available"
            })
            
            result = {
                "cameras": cameras,
                "total_count": len(cameras),
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"üì∑ Vision Tools: –ù–∞–π–¥–µ–Ω–æ –∫–∞–º–µ—Ä - {len(cameras)}")
            return f"‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞–º–µ—Ä—ã: {json.dumps(result, ensure_ascii=False)}"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∫–∞–º–µ—Ä: {e}"
            self.logger.error(error_msg)
            return error_msg
    
    def get_camera_status(self, camera_id: str = "default") -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∫–∞–º–µ—Ä—ã
        
        Args:
            camera_id: ID –∫–∞–º–µ—Ä—ã
            
        Returns:
            str: –°—Ç–∞—Ç—É—Å –∫–∞–º–µ—Ä—ã
        """
        try:
            if camera_id == "default":
                source = 0
            else:
                source = camera_id
            
            cap = cv2.VideoCapture(source)
            if not cap.isOpened():
                return f"‚ùå –ö–∞–º–µ—Ä–∞ {camera_id} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–º–µ—Ä–µ
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä
            ret, frame = cap.read()
            cap.release()
            
            status = {
                "camera_id": camera_id,
                "available": ret,
                "resolution": f"{width}x{height}",
                "fps": fps,
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"üì∑ Vision Tools: –°—Ç–∞—Ç—É—Å –∫–∞–º–µ—Ä—ã {camera_id} - {ret}")
            return f"‚úÖ –°—Ç–∞—Ç—É—Å –∫–∞–º–µ—Ä—ã: {json.dumps(status, ensure_ascii=False)}"
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–µ—Ä—ã: {e}"
            self.logger.error(error_msg)
            return error_msg

    # ===== Cloud Vision Adapters (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π SDK) =====
    def vision_labels(self, image_path: str, max_results: int = 10) -> str:
        try:
            if not self._gcv_client:
                return "‚ùå Cloud Vision SDK not configured (set GOOGLE_APPLICATION_CREDENTIALS)"
            if not os.path.exists(image_path):
                return f"‚ùå File not found: {image_path}"
            from google.cloud import vision as gcv
            with open(image_path, 'rb') as f:
                content = f.read()
            image = gcv.Image(content=content)
            response = self._gcv_client.label_detection(image=image, max_results=max_results)
            labels = [l.description for l in response.label_annotations or []]
            return "‚úÖ GV Labels: " + ", ".join(labels)
        except Exception as e:
            return f"‚ùå GV Labels error: {e}"

    def vision_objects(self, image_path: str, max_results: int = 10) -> str:
        try:
            if not self._gcv_client:
                return "‚ùå Cloud Vision SDK not configured (set GOOGLE_APPLICATION_CREDENTIALS)"
            if not os.path.exists(image_path):
                return f"‚ùå File not found: {image_path}"
            from google.cloud import vision as gcv
            with open(image_path, 'rb') as f:
                content = f.read()
            image = gcv.Image(content=content)
            response = self._gcv_client.object_localization(image=image)
            objs = []
            for o in response.localized_object_annotations or []:
                pts = [(round(v.x,3), round(v.y,3)) for v in o.bounding_poly.normalized_vertices]
                objs.append(f"{o.name}:{pts}")
                if len(objs) >= max_results:
                    break
            return "‚úÖ GV Objects: " + ("; ".join(objs) if objs else "none")
        except Exception as e:
            return f"‚ùå GV Objects error: {e}"

    def vision_ocr(self, image_path: str) -> str:
        try:
            if not self._gcv_client:
                return "‚ùå Cloud Vision SDK not configured (set GOOGLE_APPLICATION_CREDENTIALS)"
            if not os.path.exists(image_path):
                return f"‚ùå File not found: {image_path}"
            from google.cloud import vision as gcv
            with open(image_path, 'rb') as f:
                content = f.read()
            image = gcv.Image(content=content)
            response = self._gcv_client.text_detection(image=image)
            annotations = response.text_annotations or []
            text = annotations[0].description if annotations else ""
            return "‚úÖ GV OCR: " + (text.strip()[:2000])
        except Exception as e:
            return f"‚ùå GV OCR error: {e}"

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
vision_tools = VisionTools() 