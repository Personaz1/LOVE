"""
Gemini API –∫–ª–∏–µ–Ω—Ç
"""

import os
import time
import logging
from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import json

import google.generativeai as genai
from google.cloud import vision

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()

class GeminiClient:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–∞"""
        self.config = Config()
        self.error_handler = ErrorHandler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini
        api_key = self.config.get_gemini_api_key()
        genai.configure(api_key=api_key)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Vision API
        self.vision_client = None
        try:
            vision_api_key = self.config.get_vision_api_key()
            if vision_api_key:
                self.vision_api_key = vision_api_key
                logger.info("‚úÖ Google Cloud Vision API key configured")
            else:
                logger.warning("‚ö†Ô∏è No Google Cloud Vision API key provided")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google Cloud Vision API not available: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.models = [
            {
                'name': 'gemini-2.5-pro',
                'quota': 100,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-1.5-pro',
                'quota': 150,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.5-flash',
                'quota': 250,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-1.5-flash',
                'quota': 500,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.0-flash', 
                'quota': 200,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.0-flash-lite',
                'quota': 1000,
                'model': None,
                'vision': False
            },
            {
                'name': 'gemini-2.5-flash-lite',
                'quota': 1000,
                'model': None,
                'vision': False
            }
        ]
        
        self.current_model_index = 0
    
    def get_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"""
        return self.models
    
    def _get_current_model(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.models[self.current_model_index]['name']
    
    def _get_current_model_index(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self.current_model_index
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å"""
        self.current_model_index = (self.current_model_index + 1) % len(self.models)
        model_name = self.models[self.current_model_index]['name']
        logger.info(f"üöÄ Using model: {model_name}")
        return model_name
    
    def switch_to_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.models):
            if model['name'] == model_name:
                self.current_model_index = i
                logger.info(f"üöÄ Switched to model: {model_name}")
                return True
        logger.error(f"‚ùå Model {model_name} not found")
        return False
    
    def _handle_quota_error(self, error_msg: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∫–≤–æ—Ç—ã"""
        if self.error_handler.is_quota_error(error_msg):
            logger.warning(f"‚ö†Ô∏è Quota exceeded for {self._get_current_model()}, switching model")
            return self._switch_to_next_model()
        return None
    
    def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π"""
        current_model = self._get_current_model()
        current_quota = "undefined"  # –ë—É–¥–µ–º –ø–æ–ª—É—á–∞—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
        available_models = []
        model_errors = 0
        
        for i, model in enumerate(self.models):
            model_info = {
                'name': model['name'],
                'quota': model.get('quota', 'undefined'),
                'has_error': False  # –ë—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            }
            available_models.append(model_info)
        
        return {
            'current_model': current_model,
            'current_quota': current_quota,
            'model_index': self.current_model_index,
            'total_models': len(self.models),
            'available_models': available_models,
            'model_errors': model_errors
        }
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self._get_current_model()
    
    def _analyze_image_with_vision_api(self, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Vision API"""
        try:
            if not self.config.is_vision_configured():
                return "‚ùå Vision API not configured"
            
            # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Vision API
            import requests
            
            url = f"https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}"
            
            request_data = {
                "requests": [
                    {
                        "image": {
                            "content": content.decode('latin1')
                        },
                        "features": [
                            {
                                "type": "LABEL_DETECTION",
                                "maxResults": 10
                            },
                            {
                                "type": "TEXT_DETECTION"
                            },
                            {
                                "type": "FACE_DETECTION"
                            },
                            {
                                "type": "OBJECT_LOCALIZATION",
                                "maxResults": 5
                            }
                        ]
                    }
                ]
            }
            
            response = requests.post(url, json=request_data)
            
            if response.status_code == 200:
                result = response.json()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                analysis = []
                
                # Labels
                if 'labelAnnotations' in result['responses'][0]:
                    labels = [label['description'] for label in result['responses'][0]['labelAnnotations']]
                    analysis.append(f"Labels: {', '.join(labels)}")
                
                # Text
                if 'textAnnotations' in result['responses'][0]:
                    text = result['responses'][0]['textAnnotations'][0]['description']
                    analysis.append(f"Text: {text}")
                
                # Faces
                if 'faceAnnotations' in result['responses'][0]:
                    faces = len(result['responses'][0]['faceAnnotations'])
                    analysis.append(f"Faces detected: {faces}")
                
                # Objects
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    objects = [obj['name'] for obj in result['responses'][0]['localizedObjectAnnotations']]
                    analysis.append(f"Objects: {', '.join(objects)}")
                
                return " | ".join(analysis)
            else:
                return f"‚ùå Vision API error: {response.status_code}"
                
        except Exception as e:
            return f"‚ùå Vision API error: {str(e)}"
    
    async def generate_streaming_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è streaming –æ—Ç–≤–µ—Ç–∞"""
        async for chunk in self._generate_gemini_streaming_response(
            system_prompt, user_message, context, user_profile
        ):
            yield chunk
    
    async def _generate_gemini_streaming_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è streaming –æ—Ç–≤–µ—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(full_prompt, stream=True)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    yield "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                    return
                elif finish_reason == 2:  # RECITATION
                    yield "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                    return
                elif finish_reason == 3:  # OTHER
                    yield "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
                    return
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º streaming –æ—Ç–≤–µ—Ç
            for chunk in response:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                if hasattr(chunk, 'text') and chunk.text:
                    yield chunk.text
                elif hasattr(chunk, 'parts') and chunk.parts:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —á–µ—Ä–µ–∑ parts
                    for part in chunk.parts:
                        if hasattr(part, 'text') and part.text:
                            yield part.text
                elif hasattr(chunk, 'candidates') and chunk.candidates:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ candidates
                    for candidate in chunk.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            if hasattr(candidate.content, 'parts') and candidate.content.parts:
                                for part in candidate.content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        yield part.text
                    
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Gemini streaming error: {error_msg}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self._handle_quota_error(error_msg):
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                async for chunk in self._generate_gemini_streaming_response(
                    system_prompt, user_message, context, user_profile
                ):
                    yield chunk
            else:
                yield f"‚ùå Error: {error_msg}"
    
    async def _generate_gemini_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ã—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(full_prompt)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                elif finish_reason == 2:  # RECITATION
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
            if hasattr(response, 'text') and response.text:
                return response.text
            elif hasattr(response, 'parts') and response.parts:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —á–µ—Ä–µ–∑ parts
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    return "".join(text_parts)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            return "".join(text_parts)
            
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Gemini non-streaming error: {error_msg}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self._handle_quota_error(error_msg):
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                return await self._generate_gemini_response(
                    system_prompt, user_message, context, user_profile
                )
            else:
                return f"‚ùå Error: {error_msg}"
    
    def chat(
        self, 
        message: str, 
        user_profile: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —á–∞—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not system_prompt:
                system_prompt = "You are a helpful AI assistant."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —á–µ—Ä–µ–∑ sync –≤–µ—Ä—Å–∏—é
            response = self._generate_gemini_response_sync(
                system_prompt, message, conversation_context, user_profile
            )
            
            return response
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Chat error: {error_msg}")
            return f"‚ùå Error: {error_msg}"

    def _generate_gemini_response_sync(
        self, 
        system_prompt: str, 
        user_message: str, 
        conversation_context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt_sync(system_prompt, user_message, conversation_context, user_profile)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.8,
                "top_k": 40,
                "max_output_tokens": 2048,
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                elif finish_reason == 2:  # RECITATION
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã
            text_content = ""
            
            # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ text
            if hasattr(response, 'text') and response.text:
                text_content = response.text
            # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ parts
            elif hasattr(response, 'parts') and response.parts:
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    text_content = "".join(text_parts)
            # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ candidates
            elif hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            text_content = "".join(text_parts)
                    elif hasattr(candidate.content, 'text') and candidate.content.text:
                        text_content = candidate.content.text
            
            if text_content:
                return text_content
            
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
    
    def chat(
        self, 
        message: str, 
        user_profile: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —á–∞—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not system_prompt:
                system_prompt = "You are a helpful AI assistant."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —á–µ—Ä–µ–∑ sync –≤–µ—Ä—Å–∏—é
            response = self._generate_gemini_response_sync(
                system_prompt, message, conversation_context, user_profile
            )
            
            return response
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Chat error: {error_msg}")
            return f"‚ùå Error: {error_msg}"

    def _generate_gemini_response_sync(
        self, 
        system_prompt: str, 
        user_message: str, 
        conversation_context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt_sync(system_prompt, user_message, conversation_context, user_profile)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            generation_config = {
                "temperature": 0.7,
                "top_p": 0.8,
                "top_k": 40,
                "max_output_tokens": 2048,
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(
                full_prompt,
                generation_config=generation_config
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                elif finish_reason == 2:  # RECITATION
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
            if hasattr(response, 'text') and response.text:
                return response.text
            elif hasattr(response, 'parts') and response.parts:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã —á–µ—Ä–µ–∑ parts
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    return "".join(text_parts)
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑ candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            return "".join(text_parts)
            
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Sync generation error: {error_msg}")
            
            # –ï—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç –∏–ª–∏ –∫–≤–æ—Ç–∞ - –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
            if "504" in error_msg or "Deadline" in error_msg or "429" in error_msg or "quota" in error_msg.lower():
                logger.warning("‚ö†Ô∏è API –æ—à–∏–±–∫–∞ (—Ç–∞–π–º–∞—É—Ç/–∫–≤–æ—Ç–∞), –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å...")
                return self._try_fallback_model_sync(system_prompt, user_message, conversation_context, user_profile)
            
            return f"‚ùå Error: {error_msg}"

    def _build_prompt_sync(
        self, 
        system_prompt: str, 
        user_message: str, 
        conversation_context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞"""
        prompt_parts = [system_prompt]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_profile:
            prompt_parts.append(f"\nUser Profile: {json.dumps(user_profile, ensure_ascii=False)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        if conversation_context:
            prompt_parts.append(f"\nConversation Context: {conversation_context}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        prompt_parts.append(f"\nUser: {user_message}")
        prompt_parts.append("\nAssistant:")
        
        return "\n".join(prompt_parts)

    def _try_fallback_model_sync(
        self, 
        system_prompt: str, 
        user_message: str, 
        conversation_context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö API"""
        current_index = self._get_current_model_index()
        
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏, –Ω–∞—á–∏–Ω–∞—è —Å–æ —Å–ª–µ–¥—É—é—â–µ–π
        for attempt in range(len(self.models) - 1):  # -1 —á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–±–æ–≤–∞—Ç—å —Ç—É –∂–µ –º–æ–¥–µ–ª—å
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                next_index = (current_index + attempt + 1) % len(self.models)
                fallback_model_dict = self.models[next_index]
                fallback_model_name = fallback_model_dict['name']
                
                logger.info(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –º–æ–¥–µ–ª—å: {fallback_model_name}")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
                full_prompt = self._build_prompt_sync(system_prompt, user_message, conversation_context, user_profile)
                
                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å fallback
                model = genai.GenerativeModel(fallback_model_name)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
                generation_config = {
                    "temperature": 0.5,
                    "top_p": 0.7,
                    "top_k": 30,
                    "max_output_tokens": 1024,
                }
                
                response = model.generate_content(
                    full_prompt,
                    generation_config=generation_config
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason –∏ text
                if hasattr(response, 'finish_reason') and response.finish_reason == 1:  # SAFETY
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {fallback_model_name} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
                    continue
                elif response.text:
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å: {fallback_model_name}")
                    return response.text
                    
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {fallback_model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {error_msg}")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        logger.error("‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
        return "‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    def _build_prompt(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        prompt_parts = []
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_parts.append(system_prompt)
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_profile:
            profile_text = f"User profile: {json.dumps(user_profile, indent=2)}"
            prompt_parts.append(profile_text)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context:
            prompt_parts.append(f"Context: {context}")
        
        # –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        prompt_parts.append(f"User: {user_message}")
        
        return "\n\n".join(prompt_parts) 