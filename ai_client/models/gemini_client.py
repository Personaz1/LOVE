"""
Gemini API –∫–ª–∏–µ–Ω—Ç
"""

import os
import time
import logging
from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import json

import google.generativeai as genai
from google.cloud import vision

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()

class GeminiClient:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–∞"""
        self.config = Config()
        self.error_handler = ErrorHandler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini
        api_key = self.config.get_gemini_api_key()
        genai.configure(api_key=api_key)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Vision API
        self.vision_client = None
        try:
            vision_api_key = self.config.get_vision_api_key()
            if vision_api_key:
                self.vision_api_key = vision_api_key
                logger.info("‚úÖ Google Cloud Vision API key configured")
            else:
                logger.warning("‚ö†Ô∏è No Google Cloud Vision API key provided")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google Cloud Vision API not available: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.models = [
            {
                'name': 'gemini-2.5-pro',
                'quota': 100,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-1.5-pro',
                'quota': 150,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.5-flash',
                'quota': 250,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-1.5-flash',
                'quota': 500,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.0-flash', 
                'quota': 200,
                'model': None,
                'vision': True
            },
            {
                'name': 'gemini-2.0-flash-lite',
                'quota': 1000,
                'model': None,
                'vision': False
            },
            {
                'name': 'gemini-2.5-flash-lite',
                'quota': 1000,
                'model': None,
                'vision': False
            }
        ]
        
        self.current_model_index = 0
    
    def get_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"""
        return self.models
    
    def _get_current_model(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.models[self.current_model_index]['name']
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å"""
        self.current_model_index = (self.current_model_index + 1) % len(self.models)
        model_name = self.models[self.current_model_index]['name']
        logger.info(f"üöÄ Using model: {model_name}")
        return model_name
    
    def switch_to_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.models):
            if model['name'] == model_name:
                self.current_model_index = i
                logger.info(f"üöÄ Switched to model: {model_name}")
                return True
        logger.error(f"‚ùå Model {model_name} not found")
        return False
    
    def _handle_quota_error(self, error_msg: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∫–≤–æ—Ç—ã"""
        if self.error_handler.is_quota_error(error_msg):
            logger.warning(f"‚ö†Ô∏è Quota exceeded for {self._get_current_model()}, switching model")
            return self._switch_to_next_model()
        return None
    
    def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π"""
        return {
            'current_model': self._get_current_model(),
            'model_index': self.current_model_index,
            'total_models': len(self.models),
            'models': [model['name'] for model in self.models]
        }
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self._get_current_model()
    
    def _analyze_image_with_vision_api(self, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Vision API"""
        try:
            if not self.config.is_vision_configured():
                return "‚ùå Vision API not configured"
            
            # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Vision API
            import requests
            
            url = f"https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}"
            
            request_data = {
                "requests": [
                    {
                        "image": {
                            "content": content.decode('latin1')
                        },
                        "features": [
                            {
                                "type": "LABEL_DETECTION",
                                "maxResults": 10
                            },
                            {
                                "type": "TEXT_DETECTION"
                            },
                            {
                                "type": "FACE_DETECTION"
                            },
                            {
                                "type": "OBJECT_LOCALIZATION",
                                "maxResults": 5
                            }
                        ]
                    }
                ]
            }
            
            response = requests.post(url, json=request_data)
            
            if response.status_code == 200:
                result = response.json()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                analysis = []
                
                # Labels
                if 'labelAnnotations' in result['responses'][0]:
                    labels = [label['description'] for label in result['responses'][0]['labelAnnotations']]
                    analysis.append(f"Labels: {', '.join(labels)}")
                
                # Text
                if 'textAnnotations' in result['responses'][0]:
                    text = result['responses'][0]['textAnnotations'][0]['description']
                    analysis.append(f"Text: {text}")
                
                # Faces
                if 'faceAnnotations' in result['responses'][0]:
                    faces = len(result['responses'][0]['faceAnnotations'])
                    analysis.append(f"Faces detected: {faces}")
                
                # Objects
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    objects = [obj['name'] for obj in result['responses'][0]['localizedObjectAnnotations']]
                    analysis.append(f"Objects: {', '.join(objects)}")
                
                return " | ".join(analysis)
            else:
                return f"‚ùå Vision API error: {response.status_code}"
                
        except Exception as e:
            return f"‚ùå Vision API error: {str(e)}"
    
    async def generate_streaming_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è streaming –æ—Ç–≤–µ—Ç–∞"""
        async for chunk in self._generate_gemini_streaming_response(
            system_prompt, user_message, context, user_profile
        ):
            yield chunk
    
    async def _generate_gemini_streaming_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –¥–ª—è streaming –æ—Ç–≤–µ—Ç–æ–≤"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(full_prompt, stream=True)
            
            for chunk in response:
                if chunk.text:
                    yield chunk.text
                    
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Gemini streaming error: {error_msg}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self._handle_quota_error(error_msg):
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                async for chunk in self._generate_gemini_streaming_response(
                    system_prompt, user_message, context, user_profile
                ):
                    yield chunk
            else:
                yield f"‚ùå Error: {error_msg}"
    
    async def _generate_gemini_response(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ã—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content(full_prompt)
            
            return response.text
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Gemini non-streaming error: {error_msg}")
            
            # –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –º–æ–¥–µ–ª—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self._handle_quota_error(error_msg):
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                return await self._generate_gemini_response(
                    system_prompt, user_message, context, user_profile
                )
            else:
                return f"‚ùå Error: {error_msg}"
    
    def chat(
        self, 
        message: str, 
        user_profile: Optional[Dict[str, Any]] = None,
        conversation_context: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —á–∞—Ç–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not system_prompt:
                system_prompt = "You are a helpful AI assistant."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            import asyncio
            response = asyncio.run(self._generate_gemini_response(
                system_prompt, message, conversation_context, user_profile
            ))
            
            return response
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Chat error: {error_msg}")
            return f"‚ùå Error: {error_msg}"
    
    def _build_prompt(
        self, 
        system_prompt: str, 
        user_message: str, 
        context: Optional[str] = None,
        user_profile: Optional[Dict[str, Any]] = None
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞"""
        prompt_parts = []
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt_parts.append(system_prompt)
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_profile:
            profile_text = f"User profile: {json.dumps(user_profile, indent=2)}"
            prompt_parts.append(profile_text)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        if context:
            prompt_parts.append(f"Context: {context}")
        
        # –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        prompt_parts.append(f"User: {user_message}")
        
        return "\n\n".join(prompt_parts) 