"""
Gemini API –∫–ª–∏–µ–Ω—Ç - –£–ü–†–û–©–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
"""

import os
import time
import logging
from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import json

import google.generativeai as genai
from google.cloud import vision

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()

class GeminiClient:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–∞"""
        self.config = Config()
        self.error_handler = ErrorHandler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini
        api_key = self.config.get_gemini_api_key()
        genai.configure(api_key=api_key)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Vision API
        self.vision_client = None
        try:
            vision_api_key = self.config.get_vision_api_key()
            if vision_api_key:
                self.vision_api_key = vision_api_key
                logger.info("‚úÖ Google Cloud Vision API key configured")
            else:
                logger.warning("‚ö†Ô∏è No Google Cloud Vision API key provided")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google Cloud Vision API not available: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –±—ã—Å—Ç—Ä—ã–º –∏ –¥–µ—à—ë–≤—ã–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.models = [
            {'name': 'gemini-2.0-flash', 'quota': 200},         # DEFAULT
            {'name': 'gemini-2.5-flash', 'quota': 250},
            {'name': 'gemini-1.5-flash', 'quota': 500},
            {'name': 'gemini-2.0-flash-lite', 'quota': 1000},
            {'name': 'gemini-2.5-flash-lite', 'quota': 1000},
            {'name': 'gemini-2.5-pro', 'quota': 100},
            {'name': 'gemini-1.5-pro', 'quota': 150}
        ]

        self.current_model_index = 0  # gemini-2.0-flash
    
    def _parse_gemini_response(self, response) -> str:
        """–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Gemini"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    logger.warning("‚ö†Ô∏è Gemini response blocked by safety - trying to extract partial content")
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —á–∞—Å—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤–º–µ—Å—Ç–æ –ø–æ–ª–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                elif finish_reason == 2:  # RECITATION
                    logger.warning("‚ö†Ô∏è Gemini response blocked by recitation")
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    logger.warning("‚ö†Ô∏è Gemini response blocked by other reasons")
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
                elif finish_reason == 12:  # SAFETY_BLOCK
                    logger.warning("‚ö†Ô∏è Gemini response blocked by safety block")
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (SAFETY_BLOCK)"
            
            # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ text
            if hasattr(response, 'text') and response.text:
                return response.text
            
            # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ parts
            if hasattr(response, 'parts') and response.parts:
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    return "".join(text_parts)
            
            # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            return "".join(text_parts)
                    elif hasattr(candidate.content, 'text') and candidate.content.text:
                        return candidate.content.text
            
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing Gemini response: {e}")
            return f"‚ùå Error parsing response: {str(e)}"
    
    def _get_current_model(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.models[self.current_model_index]['name']
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å"""
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª–∏ —Å –Ω–∏–∑–∫–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –µ—Å–ª–∏ –µ—Å—Ç—å –ª—É—á—à–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
        current_quota = self.models[self.current_model_index]['quota']
        
        # –ò—â–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å —Å –ø–æ—Ö–æ–∂–∏–º –∏–ª–∏ –ª—É—á—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º
        for i in range(1, len(self.models)):
            next_index = (self.current_model_index + i) % len(self.models)
            next_quota = self.models[next_index]['quota']
            
            # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∞—è –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –ø–æ—Ö–æ–∂–∏–π –∏–ª–∏ –ª—É—á—à–∏–π –ª–∏–º–∏—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            if next_quota >= current_quota * 0.5:  # –ù–µ –ø–∞–¥–∞–µ–º —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ
                self.current_model_index = next_index
                model_name = self.models[self.current_model_index]['name']
                logger.info(f"üöÄ Switched to model: {model_name} (quota: {next_quota})")
                return model_name
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é, –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
        self.current_model_index = (self.current_model_index + 1) % len(self.models)
        model_name = self.models[self.current_model_index]['name']
        logger.info(f"üöÄ Using model: {model_name}")
        return model_name
    
    def switch_to_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.models):
            if model['name'] == model_name:
                self.current_model_index = i
                logger.info(f"üöÄ Switched to model: {model_name}")
                return True
        logger.error(f"‚ùå Model {model_name} not found")
        return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π"""
        current_model = self._get_current_model()
        
        available_models = []
        for i, model in enumerate(self.models):
            model_info = {
                'name': model['name'],
                'quota': model.get('quota', 'undefined'),
                'has_error': False
            }
            available_models.append(model_info)
        
        return {
            'current_model': current_model,
            'current_quota': "undefined",
            'model_index': self.current_model_index,
            'total_models': len(self.models),
            'available_models': available_models,
            'model_errors': 0
        }
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self._get_current_model()
    
    def _build_prompt(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> str:
        """–°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å reasoning –∏ chain of thoughts"""
        prompt_parts = []
        
        # System prompt
        prompt_parts.append(system_prompt)
        
        # Context if provided
        if context:
            prompt_parts.append(f"\n**CONTEXT:**\n{context}")
        
        # User profile if provided
        if user_profile:
            prompt_parts.append(f"\n**USER PROFILE:**\n{json.dumps(user_profile, indent=2)}")
        
        # User message
        prompt_parts.append(f"\n**USER MESSAGE:**\n{user_message}")
        

        
        return "\n".join(prompt_parts)
    
    async def generate_streaming_response(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None, image_path: Optional[str] = None) -> AsyncGenerator[str, None]:
        """Streaming –æ—Ç–≤–µ—Ç - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫ –ø—Ä–æ–º–ø—Ç—É
            if image_path and os.path.exists(image_path):
                try:
                    with open(image_path, 'rb') as img_file:
                        image_data = img_file.read()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME —Ç–∏–ø
                    mime_type = "image/jpeg"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    if image_path.lower().endswith('.png'):
                        mime_type = "image/png"
                    elif image_path.lower().endswith('.gif'):
                        mime_type = "image/gif"
                    elif image_path.lower().endswith('.webp'):
                        mime_type = "image/webp"
                    
                    # –°–æ–∑–¥–∞–µ–º parts —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Ç–µ–∫—Å—Ç–æ–º
                    parts = [
                        {"mime_type": mime_type, "data": image_data},
                        {"text": full_prompt}
                    ]
                    
                    response = model.generate_content(parts, stream=True)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to load image {image_path}: {e}")
                    # Fallback –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ä–µ–∂–∏–º—É
                    response = model.generate_content(full_prompt, stream=True)
            else:
                response = model.generate_content(full_prompt, stream=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º streaming –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
            for chunk in response:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk
                chunk_text = self._parse_gemini_response(chunk)
                if chunk_text and not chunk_text.startswith("‚ùå"):
                    yield chunk_text
                    
        except Exception as e:
            error_msg = str(e)
            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."
            logger.error(f"‚ùå Gemini streaming error: {error_msg}")
            
            # –ü—Ä–æ—Å—Ç–æ–π fallback - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
            if "429" in error_msg or "quota" in error_msg.lower():
                logger.warning("‚ö†Ô∏è Quota exceeded, switching model...")
                self._switch_to_next_model()
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                async for chunk in self.generate_streaming_response(system_prompt, user_message, context, user_profile):
                    yield chunk
            else:
                yield f"‚ùå Error: {error_msg}"
    
    def chat(self, message: str, user_profile: Optional[Dict[str, Any]] = None, conversation_context: Optional[str] = None, system_prompt: Optional[str] = None, image_path: Optional[str] = None) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —á–∞—Ç–∞ - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            if not system_prompt:
                system_prompt = "You are a helpful AI assistant."
            
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            full_prompt = self._build_prompt(system_prompt, message, conversation_context, user_profile)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫ –ø—Ä–æ–º–ø—Ç—É
            if image_path and os.path.exists(image_path):
                try:
                    with open(image_path, 'rb') as img_file:
                        image_data = img_file.read()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º MIME —Ç–∏–ø
                    mime_type = "image/jpeg"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    if image_path.lower().endswith('.png'):
                        mime_type = "image/png"
                    elif image_path.lower().endswith('.gif'):
                        mime_type = "image/gif"
                    elif image_path.lower().endswith('.webp'):
                        mime_type = "image/webp"
                    
                    # –°–æ–∑–¥–∞–µ–º parts —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Ç–µ–∫—Å—Ç–æ–º
                    parts = [
                        {"mime_type": mime_type, "data": image_data},
                        {"text": full_prompt}
                    ]
                    
                    response = model.generate_content(parts)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to load image {image_path}: {e}")
                    # Fallback –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ä–µ–∂–∏–º—É
                    response = model.generate_content(full_prompt)
            else:
                response = model.generate_content(full_prompt)
            
            return self._parse_gemini_response(response)
            
        except Exception as e:
            error_msg = str(e)
            # –°–æ–∫—Ä–∞—â–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
            if len(error_msg) > 200:
                error_msg = error_msg[:200] + "..."
            logger.error(f"‚ùå Chat error: {error_msg}")
            
            # –ü—Ä–æ—Å—Ç–æ–π fallback
            if "429" in error_msg or "quota" in error_msg.lower():
                logger.warning("‚ö†Ô∏è Quota exceeded, switching model...")
                self._switch_to_next_model()
                return self.chat(message, user_profile, conversation_context, system_prompt)
            else:
                return f"‚ùå Error: {error_msg}"
    
    def _analyze_image_with_vision_api(self, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Vision API"""
        try:
            if not self.config.is_vision_configured():
                return "‚ùå Vision API not configured"
            
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            import requests
            url = f"https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}"
            
            request_data = {
                "requests": [
                    {
                        "image": {"content": content.decode('latin1')},
                        "features": [
                            {"type": "LABEL_DETECTION", "maxResults": 10},
                            {"type": "TEXT_DETECTION"},
                            {"type": "FACE_DETECTION"},
                            {"type": "OBJECT_LOCALIZATION", "maxResults": 5}
                        ]
                    }
                ]
            }
            
            response = requests.post(url, json=request_data)
            
            if response.status_code == 200:
                result = response.json()
                analysis = []
                
                if 'labelAnnotations' in result['responses'][0]:
                    labels = [label['description'] for label in result['responses'][0]['labelAnnotations']]
                    analysis.append(f"Labels: {', '.join(labels)}")
                
                if 'textAnnotations' in result['responses'][0]:
                    text = result['responses'][0]['textAnnotations'][0]['description']
                    analysis.append(f"Text: {text}")
                
                if 'faceAnnotations' in result['responses'][0]:
                    faces = len(result['responses'][0]['faceAnnotations'])
                    analysis.append(f"Faces detected: {faces}")
                
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    objects = [obj['name'] for obj in result['responses'][0]['localizedObjectAnnotations']]
                    analysis.append(f"Objects: {', '.join(objects)}")
                
                return " | ".join(analysis)
            else:
                return f"‚ùå Vision API error: {response.status_code}"
                
        except Exception as e:
            return f"‚ùå Vision API error: {str(e)}" 