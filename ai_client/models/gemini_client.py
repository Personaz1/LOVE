"""
Gemini API –∫–ª–∏–µ–Ω—Ç - –£–ü–†–û–©–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
"""

import base64
import os
import time
import logging
from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import json

import google.generativeai as genai
from google.cloud import vision

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()

class GeminiClient:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–∞"""
        self.config = Config()
        self.error_handler = ErrorHandler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini
        api_key = self.config.get_gemini_api_key()
        genai.configure(api_key=api_key)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Vision API
        self.vision_client = None
        self.vision_api_key = None
        try:
            vision_api_key = self.config.get_vision_api_key()
            if vision_api_key:
                self.vision_api_key = vision_api_key
                self.vision_client = True  # Mark as available
                logger.info("‚úÖ Google Cloud Vision API key configured")
            else:
                logger.warning("‚ö†Ô∏è No Google Cloud Vision API key provided")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google Cloud Vision API not available: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.models = [
            {'name': 'gemini-2.5-pro', 'quota': 100},
            {'name': 'gemini-1.5-pro', 'quota': 150},
            {'name': 'gemini-2.5-flash', 'quota': 250},
            {'name': 'gemini-1.5-flash', 'quota': 500},
            {'name': 'gemini-2.0-flash', 'quota': 200},
            {'name': 'gemini-2.0-flash-lite', 'quota': 1000},
            {'name': 'gemini-2.5-flash-lite', 'quota': 1000}
        ]
        
        self.current_model_index = 0
    
    def _parse_gemini_response(self, response) -> str:
        """–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Gemini"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                if finish_reason == 1:  # SAFETY
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                elif finish_reason == 2:  # RECITATION
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
            
            # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ text
            if hasattr(response, 'text') and response.text:
                return response.text
            
            # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ parts
            if hasattr(response, 'parts') and response.parts:
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    return "".join(text_parts)
            
            # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ candidates
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            return "".join(text_parts)
                    elif hasattr(candidate.content, 'text') and candidate.content.text:
                        return candidate.content.text
            
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
            
        except Exception as e:
            logger.error(f"‚ùå Parse error: {e}")
            return f"‚ùå Parse error: {str(e)}"
    
    def _get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.models[self.current_model_index]['name']
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å"""
        self.current_model_index = (self.current_model_index + 1) % len(self.models)
        logger.info(f"üöÄ Using model: {self._get_current_model()}")
    
    def switch_to_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.models):
            if model['name'] == model_name:
                self.current_model_index = i
                logger.info(f"üöÄ Switched to model: {model_name}")
                return True
        return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π"""
        current_model = self.models[self.current_model_index]
        return {
            'current_model': current_model['name'],
            'current_quota': current_model['quota'],
            'available_models': [model['name'] for model in self.models],
            'model_index': self.current_model_index
        }
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self._get_current_model()
    
    def _build_prompt(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> str:
        """–°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        full_prompt = system_prompt
        
        if context:
            full_prompt += f"\n\nContext: {context}"
        
        if user_profile:
            profile_info = f"User Profile: {json.dumps(user_profile, ensure_ascii=False)}"
            full_prompt += f"\n\n{profile_info}"
        
        full_prompt += f"\n\nUser: {user_message}"
        return full_prompt
    
    async def generate_streaming_response(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç"""
        try:
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
            response = model.generate_content(full_prompt, stream=True)
            
            for chunk in response:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk
                chunk_text = self._parse_gemini_response(chunk)
                if chunk_text and not chunk_text.startswith("‚ùå"):
                    yield chunk_text
                    
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Gemini streaming error: {error_msg}")
            
            # –ü—Ä–æ—Å—Ç–æ–π fallback - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
            if "429" in error_msg or "quota" in error_msg.lower():
                logger.warning("‚ö†Ô∏è Quota exceeded, switching model...")
                self._switch_to_next_model()
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–±—É–µ–º —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é
                async for chunk in self.generate_streaming_response(system_prompt, user_message, context, user_profile):
                    yield chunk
            else:
                yield f"‚ùå Error: {error_msg}"
    
    def chat(self, message: str, user_profile: Optional[Dict[str, Any]] = None, conversation_context: Optional[str] = None, system_prompt: Optional[str] = None) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —á–∞—Ç–∞ - –£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
        try:
            if not system_prompt:
                system_prompt = "You are a helpful AI assistant."
            
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            full_prompt = self._build_prompt(system_prompt, message, conversation_context, user_profile)
            response = model.generate_content(full_prompt)
            
            return self._parse_gemini_response(response)
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå Chat error: {error_msg}")
            
            # –ü—Ä–æ—Å—Ç–æ–π fallback
            if "429" in error_msg or "quota" in error_msg.lower():
                logger.warning("‚ö†Ô∏è Quota exceeded, switching model...")
                self._switch_to_next_model()
                return self.chat(message, user_profile, conversation_context, system_prompt)
            else:
                return f"‚ùå Error: {error_msg}"
    
    def _analyze_image_with_llm(self, image_path: str, prompt: str = "") -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM –º–æ–¥–µ–ª—å —Å vision"""
        try:
            import base64
            
            # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(image_path, 'rb') as image_file:
                image_data = image_file.read()
            
            # –ö–æ–¥–∏—Ä—É–µ–º –≤ base64
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å vision
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°–æ–∑–¥–∞–µ–º prompt —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if not prompt:
                prompt = "Analyze this image and describe what you see in detail."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content([
                prompt,
                {"mime_type": "image/png", "data": image_base64}
            ])
            
            return self._parse_gemini_response(response)
            
        except Exception as e:
            logger.error(f"LLM vision analysis error: {e}")
            return f"‚ùå LLM vision analysis failed: {str(e)}"
    
    def _analyze_image_with_vision_api(self, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Vision API"""
        try:
            if not self.vision_api_key:
                return "‚ùå Vision API not configured"
            
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            import requests
            url = f"https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}"
            
            request_data = {
                "requests": [
                    {
                        "image": {"content": base64.b64encode(content).decode("utf-8")},
                        "features": [
                            {"type": "LABEL_DETECTION", "maxResults": 10},
                            {"type": "TEXT_DETECTION"},
                            {"type": "FACE_DETECTION"},
                            {"type": "OBJECT_LOCALIZATION", "maxResults": 5}
                        ]
                    }
                ]
            }
            
            response = requests.post(url, json=request_data)
            
            if response.status_code == 200:
                result = response.json()
                analysis = []
                
                if 'labelAnnotations' in result['responses'][0]:
                    labels = [label['description'] for label in result['responses'][0]['labelAnnotations']]
                    analysis.append(f"Labels: {', '.join(labels)}")
                
                if 'textAnnotations' in result['responses'][0]:
                    text = result['responses'][0]['textAnnotations'][0]['description']
                    analysis.append(f"Text: {text}")
                
                if 'faceAnnotations' in result['responses'][0]:
                    faces = len(result['responses'][0]['faceAnnotations'])
                    analysis.append(f"Faces detected: {faces}")
                
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    objects = [obj['name'] for obj in result['responses'][0]['localizedObjectAnnotations']]
                    analysis.append(f"Objects: {', '.join(objects)}")
                
                return " | ".join(analysis)
            else:
                return f"‚ùå Vision API error: {response.status_code}"
                
        except Exception as e:
            return f"‚ùå Vision API error: {str(e)}" 