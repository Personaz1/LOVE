"""
Gemini API –∫–ª–∏–µ–Ω—Ç - –£–ü–†–û–©–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
"""

import base64
import os
import time
import logging
from typing import Optional, Dict, Any, AsyncGenerator, List
from datetime import datetime
import json

import google.generativeai as genai
from google.cloud import vision

from ..utils.config import Config
from ..utils.logger import Logger
from ..utils.error_handler import ErrorHandler

logger = Logger()

class GeminiClient:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Gemini API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemini –∫–ª–∏–µ–Ω—Ç–∞"""
        self.config = Config()
        self.error_handler = ErrorHandler()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini
        api_key = self.config.get_gemini_api_key()
        genai.configure(api_key=api_key)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Vision API
        self.vision_client = None
        self.vision_api_key = None
        try:
            vision_api_key = self.config.get_vision_api_key()
            if vision_api_key:
                self.vision_api_key = vision_api_key
                self.vision_client = True  # Mark as available
                logger.info("‚úÖ Google Cloud Vision API key configured")
            else:
                logger.warning("‚ö†Ô∏è No Google Cloud Vision API key provided")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Google Cloud Vision API not available: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å gemini-2.5-flash-lite –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.models = [
            {'name': 'gemini-2.5-flash-lite', 'quota': 1000},  # –ú–û–î–ï–õ–¨ –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ
            {'name': 'gemini-2.5-pro', 'quota': 100},
            {'name': 'gemini-1.5-pro', 'quota': 150},
            {'name': 'gemini-2.5-flash', 'quota': 250},
            {'name': 'gemini-1.5-flash', 'quota': 500},
            {'name': 'gemini-2.0-flash', 'quota': 200},
            {'name': 'gemini-2.0-flash-lite', 'quota': 1000}
        ]
        
        self.current_model_index = 0  # gemini-2.5-flash-lite
    
    def _parse_gemini_response(self, response) -> str:
        """–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –ü–ê–†–°–ï–† - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ª—é–±–æ–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ Gemini"""
        try:
            # –û—Ç–ª–∞–¥–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
            logger.info(f"üîç Parsing response type: {type(response)}")
            logger.info(f"üîç Response attributes: {dir(response)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º finish_reason
            if hasattr(response, 'finish_reason'):
                finish_reason = response.finish_reason
                logger.info(f"üîç Finish reason: {finish_reason}")
                if finish_reason == 1:  # SAFETY
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                elif finish_reason == 2:  # RECITATION
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ä–µ—Ü–∏—Ç–∞—Ü–∏–∏"
                elif finish_reason == 3:  # OTHER
                    return "‚ùå –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏—á–∏–Ω–∞–º"
            
            # –°–ø–æ—Å–æ–± 1: –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ text
            if hasattr(response, 'text') and response.text:
                logger.info(f"‚úÖ Found text directly: {response.text[:100]}...")
                return response.text
            
            # –°–ø–æ—Å–æ–± 2: —á–µ—Ä–µ–∑ parts
            if hasattr(response, 'parts') and response.parts:
                logger.info(f"üîç Found {len(response.parts)} parts")
                text_parts = []
                for part in response.parts:
                    if hasattr(part, 'text') and part.text:
                        text_parts.append(part.text)
                if text_parts:
                    result = "".join(text_parts)
                    logger.info(f"‚úÖ Found text via parts: {result[:100]}...")
                    return result
            
            # –°–ø–æ—Å–æ–± 3: —á–µ—Ä–µ–∑ candidates
            if hasattr(response, 'candidates') and response.candidates:
                logger.info(f"üîç Found {len(response.candidates)} candidates")
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        logger.info(f"üîç Found {len(candidate.content.parts)} content parts")
                        text_parts = []
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                text_parts.append(part.text)
                        if text_parts:
                            result = "".join(text_parts)
                            logger.info(f"‚úÖ Found text via candidates: {result[:100]}...")
                            return result
                    elif hasattr(candidate.content, 'text') and candidate.content.text:
                        logger.info(f"‚úÖ Found text via candidate content: {candidate.content.text[:100]}...")
                        return candidate.content.text
            
            logger.warning("‚ö†Ô∏è No text found in response")
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
            
        except Exception as e:
            logger.error(f"‚ùå Parse error: {e}")
            return f"‚ùå Parse error: {str(e)}"
    
    def _get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å"""
        return self.models[self.current_model_index]['name']
    
    def _switch_to_next_model(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å"""
        self.current_model_index = (self.current_model_index + 1) % len(self.models)
        logger.info(f"üöÄ Using model: {self._get_current_model()}")
    
    def switch_to_model(self, model_name: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å"""
        for i, model in enumerate(self.models):
            if model['name'] == model_name:
                self.current_model_index = i
                logger.info(f"üöÄ Switched to model: {model_name}")
                return True
        return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π"""
        current_model = self.models[self.current_model_index]
        return {
            'current_model': current_model['name'],
            'current_quota': current_model['quota'],
            'available_models': [model['name'] for model in self.models],
            'model_index': self.current_model_index
        }
    
    def get_current_model(self) -> str:
        """–ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        return self._get_current_model()
    
    def _build_prompt(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> str:
        """–°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        full_prompt = system_prompt
        
        if context:
            full_prompt += f"\n\nContext: {context}"
        
        if user_profile:
            profile_info = f"User Profile: {json.dumps(user_profile, ensure_ascii=False)}"
            full_prompt += f"\n\n{profile_info}"
        
        full_prompt += f"\n\nUser: {user_message}"
        return full_prompt
    
    async def generate_streaming_response(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º fallback"""
        
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å fallback
        for attempt in range(len(self.models)):
            try:
                model_name = self._get_current_model()
                logger.info(f"üîÑ Streaming attempt {attempt + 1}: Using model {model_name}")
                
                model = genai.GenerativeModel(model_name)
                
                full_prompt = self._build_prompt(system_prompt, user_message, context, user_profile)
                response = model.generate_content(full_prompt, stream=True)
                
                for chunk in response:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ chunk
                    chunk_text = self._parse_gemini_response(chunk)
                    if chunk_text and not chunk_text.startswith("‚ùå"):
                        yield chunk_text
                
                # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω
                logger.info(f"‚úÖ Streaming success with model {model_name}")
                return
                    
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"‚ö†Ô∏è Streaming error with model {self._get_current_model()}: {error_msg}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è fallback
                if "429" in error_msg or "quota" in error_msg.lower() or "rate limit" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Quota/rate limit exceeded, switching to next model...")
                    self._switch_to_next_model()
                    continue
                elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Safety block, switching to next model...")
                    self._switch_to_next_model()
                    continue
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ —Ç–æ–∂–µ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    logger.warning(f"‚ö†Ô∏è Other error, switching to next model...")
                    self._switch_to_next_model()
                    continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        logger.error("‚ùå All models exhausted in streaming")
        yield "‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
    
    def chat(self, message: str, user_profile: Optional[Dict[str, Any]] = None, conversation_context: Optional[str] = None, system_prompt: Optional[str] = None) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —á–∞—Ç–∞ —Å Gemini —Å reasoning —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç –∏ fallback"""
        
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å fallback
        for attempt in range(len(self.models)):
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
                model_name = self._get_current_model()
                logger.info(f"üîÑ Attempt {attempt + 1}: Using model {model_name}")
                
                model = genai.GenerativeModel(model_name)
                
                # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å reasoning
                prompt = self._build_prompt_with_reasoning(system_prompt or "You are a helpful AI assistant.", message, conversation_context, user_profile)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        top_p=0.8,
                        top_k=40,
                        max_output_tokens=8192
                    )
                )
                
                result = self._parse_gemini_response(response)
                logger.info(f"‚úÖ Success with model {model_name}, result: {result[:100]}...")
                return result
                
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"‚ö†Ô∏è Error with model {self._get_current_model()}: {error_msg}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è fallback
                if "429" in error_msg or "quota" in error_msg.lower() or "rate limit" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Quota/rate limit exceeded, switching to next model...")
                    self._switch_to_next_model()
                    continue
                elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Safety block, switching to next model...")
                    self._switch_to_next_model()
                    continue
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ —Ç–æ–∂–µ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    logger.warning(f"‚ö†Ô∏è Other error, switching to next model...")
                    self._switch_to_next_model()
                    continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        logger.error("‚ùå All models exhausted")
        return "‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É."
    
    def _build_prompt_with_reasoning(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç —Å reasoning —á–µ—Ä–µ–∑ step-by-step –∞–Ω–∞–ª–∏–∑"""
        
        reasoning_prompt = f"""
{system_prompt}

**REASONING INSTRUCTIONS:**
When responding, use this format:

ü§ñ **THOUGHT PROCESS:**
[Your step-by-step reasoning, analysis, and decision-making process]

üí¨ **FINAL RESPONSE:**
[Your actual response to the user]

**CONTEXT:**
{context or "No previous context"}

**USER PROFILE:**
{json.dumps(user_profile, indent=2) if user_profile else "No user profile"}

**USER MESSAGE:**
{user_message}

**RESPONSE FORMAT:**
Always start with ü§ñ **THOUGHT PROCESS:** followed by your reasoning, then üí¨ **FINAL RESPONSE:** with your actual answer.
"""
        
        return reasoning_prompt
    
    def _parse_gemini_response_with_thoughts(self, response) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–µ—Ä –æ—Ç–≤–µ—Ç–∞ —Å reasoning —á–∞—Å—Ç—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç"""
        try:
            result = {
                'thoughts': [],
                'final_answer': '',
                'parts': [],
                'tool_calls': []  # –î–æ–±–∞–≤–ª—è–µ–º tool_calls
            }
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ —Å –æ—Ç–ª–∞–¥–æ—á–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            logger.info(f"üîç Parsing response with thoughts...")
            
            # –ï—Å–ª–∏ response —É–∂–µ —Å—Ç—Ä–æ–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –Ω–∞–ø—Ä—è–º—É—é
            if isinstance(response, str):
                response_text = response
                logger.info(f"üîç Response is already string: {response_text[:200]}...")
            else:
                response_text = self._parse_gemini_response(response)
                logger.info(f"üîç Parsed response text: {response_text[:200]}...")
            
            # –ü–∞—Ä—Å–∏–º reasoning –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
            if "ü§ñ **THOUGHT PROCESS:**" in response_text and "üí¨ **FINAL RESPONSE:**" in response_text:
                logger.info(f"‚úÖ Found reasoning markers")
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ thought –∏ final response
                parts = response_text.split("üí¨ **FINAL RESPONSE:**")
                if len(parts) == 2:
                    thought_part = parts[0].replace("ü§ñ **THOUGHT PROCESS:**", "").strip()
                    final_part = parts[1].strip()
                    
                    logger.info(f"üîç Thought part: {thought_part[:100]}...")
                    logger.info(f"üîç Final part: {final_part[:100]}...")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º thought
                    if thought_part:
                        result['thoughts'].append(thought_part)
                        result['parts'].append({
                            'text': thought_part,
                            'thought': True
                        })
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º final response
                    if final_part:
                        result['final_answer'] = final_part
                        result['parts'].append({
                            'text': final_part,
                            'thought': False
                        })
                else:
                    logger.warning(f"‚ö†Ô∏è Could not split reasoning parts")
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–¥–µ–ª–∏—Ç—å, —Å—á–∏—Ç–∞–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ final
                    result['final_answer'] = response_text
                    result['parts'].append({
                        'text': response_text,
                        'thought': False
                    })
            else:
                logger.info(f"‚úÖ No reasoning markers, treating as final response")
                # –ï—Å–ª–∏ –Ω–µ—Ç –º–∞—Ä–∫–µ—Ä–æ–≤ reasoning, —Å—á–∏—Ç–∞–µ–º –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ final
                result['final_answer'] = response_text
                result['parts'].append({
                    'text': response_text,
                    'thought': False
                })
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º tool calls –∏–∑ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ (–∏ –∏–∑ thoughts, –∏ –∏–∑ final)
            all_text = response_text
            tool_calls = self._extract_tool_calls_from_text(all_text)
            result['tool_calls'] = tool_calls
            
            logger.info(f"‚úÖ Final result: thoughts={len(result['thoughts'])}, final_answer={len(result['final_answer'])}, tool_calls={len(tool_calls)}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error parsing response with thoughts: {e}")
            return {
                'thoughts': [],
                'final_answer': f"‚ùå Error parsing response: {str(e)}",
                'parts': [{"text": f"‚ùå Error parsing response: {str(e)}", "thought": False}],
                'tool_calls': []
            }
    
    def _extract_tool_calls_from_text(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç tool calls –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞"""
        try:
            logger.info(f"üîß TOOL EXTRACTION: Processing text: {text[:100]}...")
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã tool calls –≤ —Ç–µ–∫—Å—Ç–µ
            tool_calls = []
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω: {"tool_code": "..."}
            import re
            tool_pattern = r'\{[^}]*"tool_code"[^}]*\}'
            matches = re.findall(tool_pattern, text)
            
            for match in matches:
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
                    import json
                    
                    # –ó–∞–º–µ–Ω—è–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                    cleaned_match = match.replace('\\n', '\n').replace('\\"', '"').replace("\\'", "'")
                    
                    tool_call = json.loads(cleaned_match)
                    if 'tool_code' in tool_call:
                        tool_calls.append(tool_call)
                        logger.info(f"üîß Found tool call: {tool_call['tool_code'][:50]}...")
                except json.JSONDecodeError as e:
                    logger.warning(f"‚ö†Ô∏è Failed to parse tool call JSON: {match[:100]}... Error: {e}")
                    # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
                    try:
                        # –ò—â–µ–º tool_code –≤ —Ç–µ–∫—Å—Ç–µ –Ω–∞–ø—Ä—è–º—É—é —Å –±–æ–ª–µ–µ –≥–∏–±–∫–∏–º regex
                        tool_code_pattern = r'"tool_code":\s*"((?:[^"\\]|\\.)*)"'
                        tool_code_match = re.search(tool_code_pattern, match)
                        if tool_code_match:
                            tool_code = tool_code_match.group(1)
                            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                            tool_code = tool_code.replace('\\n', '\n').replace('\\"', '"').replace("\\'", "'")
                            tool_calls.append({"tool_code": tool_code})
                            logger.info(f"üîß Found tool call via regex: {tool_code[:50]}...")
                    except Exception as regex_error:
                        logger.warning(f"‚ö†Ô∏è Failed regex parsing: {regex_error}")
                    continue
            
            logger.info(f"üîß TOOL EXTRACTION: Final result: {len(tool_calls)} tool calls")
            return tool_calls
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting tool calls: {e}")
            return []
    
    async def generate_streaming_response_with_thoughts(self, system_prompt: str, user_message: str, context: Optional[str] = None, user_profile: Optional[Dict[str, Any]] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ —Å reasoning —á–∞—Å—Ç—è–º–∏ —á–µ—Ä–µ–∑ –ø—Ä–æ–º–ø—Ç —Å fallback"""
        
        # –ü—Ä–æ–±—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å fallback
        for attempt in range(len(self.models)):
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
                model_name = self._get_current_model()
                logger.info(f"üîÑ Streaming thoughts attempt {attempt + 1}: Using model {model_name}")
                
                model = genai.GenerativeModel(model_name)
                
                # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç —Å reasoning
                prompt = self._build_prompt_with_reasoning(system_prompt, user_message, context, user_profile)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        top_p=0.8,
                        top_k=40,
                        max_output_tokens=8192
                    ),
                    stream=True
                )
                
                # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∏–º–∏–Ω–≥ —Å thoughts
                current_text = ""
                in_thought_section = False
                
                for chunk in response:
                    if hasattr(chunk, 'candidates') and chunk.candidates:
                        candidate = chunk.candidates[0]
                        if hasattr(candidate, 'content') and candidate.content:
                            if hasattr(candidate.content, 'parts') and candidate.content.parts:
                                for part in candidate.content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        current_text += part.text
                                        
                                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞—Ä–∫–µ—Ä—ã reasoning
                                        if "ü§ñ **THOUGHT PROCESS:**" in current_text and not in_thought_section:
                                            in_thought_section = True
                                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ thought
                                            yield {
                                                'text': "ü§ñ **THOUGHT PROCESS:**",
                                                'thought': True,
                                                'type': 'thought_start'
                                            }
                                        elif "üí¨ **FINAL RESPONSE:**" in current_text and in_thought_section:
                                            in_thought_section = False
                                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ final response
                                            yield {
                                                'text': "üí¨ **FINAL RESPONSE:**",
                                                'thought': False,
                                                'type': 'answer_start'
                                            }
                                        else:
                                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ–±—ã—á–Ω—ã–π chunk
                                            yield {
                                                'text': part.text,
                                                'thought': in_thought_section,
                                                'type': 'thought' if in_thought_section else 'answer'
                                            }
                            elif hasattr(candidate.content, 'text') and candidate.content.text:
                                current_text += candidate.content.text
                                yield {
                                    'text': candidate.content.text,
                                    'thought': in_thought_section,
                                    'type': 'thought' if in_thought_section else 'answer'
                                }
                
                # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥ —É—Å–ø–µ—à–µ–Ω
                logger.info(f"‚úÖ Streaming thoughts success with model {model_name}")
                return
                
            except Exception as e:
                error_msg = str(e)
                logger.warning(f"‚ö†Ô∏è Streaming thoughts error with model {self._get_current_model()}: {error_msg}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è fallback
                if "429" in error_msg or "quota" in error_msg.lower() or "rate limit" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Quota/rate limit exceeded, switching to next model...")
                    self._switch_to_next_model()
                    continue
                elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è Safety block, switching to next model...")
                    self._switch_to_next_model()
                    continue
                else:
                    # –î–ª—è –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ —Ç–æ–∂–µ –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å
                    logger.warning(f"‚ö†Ô∏è Other error, switching to next model...")
                    self._switch_to_next_model()
                    continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã
        logger.error("‚ùå All models exhausted in streaming thoughts")
        yield {
            'text': "‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.",
            'thought': False,
            'type': 'error'
        }
    
    def _analyze_image_with_llm(self, image_path: str, prompt: str = "") -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM –º–æ–¥–µ–ª—å —Å vision"""
        try:
            import base64
            
            # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            with open(image_path, 'rb') as image_file:
                image_data = image_file.read()
            
            # –ö–æ–¥–∏—Ä—É–µ–º –≤ base64
            image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å vision
            model_name = self._get_current_model()
            model = genai.GenerativeModel(model_name)
            
            # –°–æ–∑–¥–∞–µ–º prompt —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
            if not prompt:
                prompt = "Analyze this image and describe what you see in detail."
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = model.generate_content([
                prompt,
                {"mime_type": "image/png", "data": image_base64}
            ])
            
            return self._parse_gemini_response(response)
            
        except Exception as e:
            logger.error(f"LLM vision analysis error: {e}")
            return f"‚ùå LLM vision analysis failed: {str(e)}"
    
    def _analyze_image_with_vision_api(self, image_path: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Vision API"""
        try:
            if not self.vision_api_key:
                return "‚ùå Vision API not configured"
            
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            import requests
            url = f"https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}"
            
            request_data = {
                "requests": [
                    {
                        "image": {"content": base64.b64encode(content).decode("utf-8")},
                        "features": [
                            {"type": "LABEL_DETECTION", "maxResults": 10},
                            {"type": "TEXT_DETECTION"},
                            {"type": "FACE_DETECTION"},
                            {"type": "OBJECT_LOCALIZATION", "maxResults": 5}
                        ]
                    }
                ]
            }
            
            response = requests.post(url, json=request_data)
            
            if response.status_code == 200:
                result = response.json()
                analysis = []
                
                if 'labelAnnotations' in result['responses'][0]:
                    labels = [label['description'] for label in result['responses'][0]['labelAnnotations']]
                    analysis.append(f"Labels: {', '.join(labels)}")
                
                if 'textAnnotations' in result['responses'][0]:
                    text = result['responses'][0]['textAnnotations'][0]['description']
                    analysis.append(f"Text: {text}")
                
                if 'faceAnnotations' in result['responses'][0]:
                    faces = len(result['responses'][0]['faceAnnotations'])
                    analysis.append(f"Faces detected: {faces}")
                
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    objects = [obj['name'] for obj in result['responses'][0]['localizedObjectAnnotations']]
                    analysis.append(f"Objects: {', '.join(objects)}")
                
                return " | ".join(analysis)
            else:
                return f"‚ùå Vision API error: {response.status_code}"
                
        except Exception as e:
            return f"‚ùå Vision API error: {str(e)}" 